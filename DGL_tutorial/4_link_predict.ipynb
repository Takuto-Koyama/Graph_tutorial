{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GNNを用いたリンク予測\n",
        "`introduction <1_introduction>`では、GNNを用いたノード分類 (グラフ内のノードの分類を予測する) について学習した。今回は、リンク予測 (グラフ内の任意の2つのノード間のエッジの存在を予測する) にGNNを訓練する方法を学ぶ。\n",
        "\n",
        "\n",
        "このチュートリアルの目標は、以下の通りである:\n",
        "\n",
        "- GNNを用いたリンク予測モデルを構築する。\n",
        "- 小規模なDGL提供のデータセットでモデルを訓練し、評価する。\n",
        "\n",
        "(Time estimate: 28 minutes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# ライブラリのインポート\n",
        "import itertools\n",
        "import os\n",
        "\n",
        "os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
        "\n",
        "import dgl\n",
        "import dgl.data\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GNNを用いたリンク予測の概要\n",
        "\n",
        "social recommendationや、item recommendation、知識グラフ補完のような多くの適用例は、リンク予測として定式化できる。リンク予測は、2つの特定のノード間にエッジが存在するかどうかを予測する。このチュートリアルでは、引用ネットワーク内の2つの論文間に引用関係が存在するかどうかを予測する例を示す。\n",
        "\n",
        "このチュートリアルでは、リンク予測問題を以下のように2値分類問題として定式化する:\n",
        "\n",
        "- グラフ内のエッジを *positive examples* として扱う。\n",
        "- 存在しないエッジ (つまり、それらの間にエッジが存在しないノードペア) を *negative examples* としてサンプリングする。\n",
        "- positive examples と negative examples をトレーニングセットとテストセットに分割する。\n",
        "- 任意の2値分類メトリック (例: Area Under Curve (AUC)) でモデルを評価する。\n",
        "\n",
        "<div class=\"alert alert-info\"><h4>Note</h4><p>The practice comes from\n",
        "   [SEAL](https://papers.nips.cc/paper/2018/file/53f0d7c537d99b3824f0f99d62ea2428-Paper.pdf)_,\n",
        "   although the model here does not use their idea of node labeling.</p></div>\n",
        "\n",
        "大規模スケールのレコメンダーシステムや情報検索のようないくつかのドメインでは、上位K個の予測の良いパフォーマンスを強調するような評価指標が望ましい。この場合、平均適合率 (mean average precision) などの他の評価指標を用いてもよく、このチュートリアルの範疇を超えるが、他のネガティブサンプリング手法を適用してもよい。\n",
        "\n",
        "## グラフと特徴量の読み込み\n",
        "`introduction <1_introduction>`に引き続き、ここでもまずCoraデータセットを読み込む。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading /home/koyama/.dgl/cora_v2.zip from https://data.dgl.ai/dataset/cora_v2.zip...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4cb8b9cca5564ca5b7ed0b52a22d6a9a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "/home/koyama/.dgl/cora_v2.zip:   0%|          | 0.00/132k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting file to /home/koyama/.dgl/cora_v2_d697a464\n",
            "Finished data loading and preprocessing.\n",
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done saving data into cached files.\n"
          ]
        }
      ],
      "source": [
        "dataset = dgl.data.CoraGraphDataset()\n",
        "g = dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare training and testing sets\n",
        "\n",
        "## 学習セットとテストセットの準備\n",
        "\n",
        "このチュートリアルでは、テストセットのpositive examplesとしてエッジの10%をランダムに選択し、残りをトレーニングセットに残す。そして、それぞれのセットに同じ数のnegative examplesをサンプリングする。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "u, v = g.edges()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "u"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# エッジセットをトレーニングとテストに分割する\n",
        "u, v = g.edges()\n",
        "\n",
        "eids = np.arange(g.num_edges())\n",
        "eids = np.random.permutation(eids)\n",
        "test_size = int(len(eids) * 0.1)\n",
        "train_size = g.num_edges() - test_size\n",
        "test_pos_u, test_pos_v = u[eids[:test_size]], v[eids[:test_size]]\n",
        "train_pos_u, train_pos_v = u[eids[test_size:]], v[eids[test_size:]]\n",
        "\n",
        "# すべての負例のエッジを見つけ、トレーニングとテストに分割します\n",
        "adj = sp.coo_matrix((np.ones(len(u)), (u.numpy(), v.numpy()))) # 隣接行列を作成 -> 補足A\n",
        "adj_neg = 1 - adj.todense() - np.eye(g.num_nodes()) # 隣接行列の補集合を作成 -> 補足B\n",
        "neg_u, neg_v = np.where(adj_neg != 0)\n",
        "\n",
        "neg_eids = np.random.choice(len(neg_u), g.num_edges()) # 負例のエッジをランダムに選択\n",
        "test_neg_u, test_neg_v = (\n",
        "    neg_u[neg_eids[:test_size]],\n",
        "    neg_v[neg_eids[:test_size]],\n",
        ")\n",
        "train_neg_u, train_neg_v = (\n",
        "    neg_u[neg_eids[test_size:]],\n",
        "    neg_v[neg_eids[test_size:]],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 補足A 隣接行列の作成\n",
        "\n",
        "`scipy.sparse.coo_matrix`では、value, (row, col)の形式でデータを入力することで、(row, col)の位置にvalueを代入した疎行列を作成できる。今回はエッジのインデックスを(u, v)に与えるため、`scipy.sparse.coo_matrix`を使って隣接行列を作成する。\n",
        "```python\n",
        "adj = sp.coo_matrix((np.ones(len(u)), (u.numpy(), v.numpy())))\n",
        "```\n",
        "\n",
        "### 補足B 隣接行列の反転\n",
        "`adj_neg`では、隣接行列`adj`が反転された行列が作成される。具体的には、1 - `adj`で隣接行列が反転され、 `-np.eye(num_nodes)`で対角成分が0になるように調整される。\n",
        "```python\n",
        "adj_neg = 1 - adj.todense() - np.eye(g.num_nodes())\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "学習するときは、テストセットのエッジを元のグラフから削除する必要がある。これは``dgl.remove_edges``を使って行うことができる。\n",
        "\n",
        "\n",
        "<div class=\"alert alert-info\"><h4>Note</h4><p>``dgl.remove_edges`` works by creating a subgraph from the\n",
        "   original graph, resulting in a copy and therefore could be slow for\n",
        "   large graphs. If so, you could save the training and test graph to\n",
        "   disk, as you would do for preprocessing.</p></div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_g = dgl.remove_edges(g, eids[:test_size]) # テストエッジを削除"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define a GraphSAGE model\n",
        "\n",
        "This tutorial builds a model consisting of two\n",
        "[GraphSAGE](https://arxiv.org/abs/1706.02216)_ layers, each computes\n",
        "new node representations by averaging neighbor information. DGL provides\n",
        "``dgl.nn.SAGEConv`` that conveniently creates a GraphSAGE layer.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from dgl.nn import SAGEConv\n",
        "\n",
        "\n",
        "# ----------- 2. create model -------------- #\n",
        "# build a two-layer GraphSAGE model\n",
        "class GraphSAGE(nn.Module):\n",
        "    def __init__(self, in_feats, h_feats):\n",
        "        super(GraphSAGE, self).__init__()\n",
        "        self.conv1 = SAGEConv(in_feats, h_feats, \"mean\")\n",
        "        self.conv2 = SAGEConv(h_feats, h_feats, \"mean\")\n",
        "\n",
        "    def forward(self, g, in_feat):\n",
        "        h = self.conv1(g, in_feat)\n",
        "        h = F.relu(h)\n",
        "        h = self.conv2(g, h)\n",
        "        return h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model then predicts the probability of existence of an edge by\n",
        "computing a score between the representations of both incident nodes\n",
        "with a function (e.g. an MLP or a dot product), which you will see in\n",
        "the next section.\n",
        "\n",
        "\\begin{align}\\hat{y}_{u\\sim v} = f(h_u, h_v)\\end{align}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Positive graph, negative graph, and ``apply_edges``\n",
        "\n",
        "In previous tutorials you have learned how to compute node\n",
        "representations with a GNN. However, link prediction requires you to\n",
        "compute representation of *pairs of nodes*.\n",
        "\n",
        "DGL recommends you to treat the pairs of nodes as another graph, since\n",
        "you can describe a pair of nodes with an edge. In link prediction, you\n",
        "will have a *positive graph* consisting of all the positive examples as\n",
        "edges, and a *negative graph* consisting of all the negative examples.\n",
        "The *positive graph* and the *negative graph* will contain the same set\n",
        "of nodes as the original graph.  This makes it easier to pass node\n",
        "features among multiple graphs for computation.  As you will see later,\n",
        "you can directly feed the node representations computed on the entire\n",
        "graph to the positive and the negative graphs for computing pair-wise\n",
        "scores.\n",
        "\n",
        "The following code constructs the positive graph and the negative graph\n",
        "for the training set and the test set respectively.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_pos_g = dgl.graph((train_pos_u, train_pos_v), num_nodes=g.num_nodes())\n",
        "train_neg_g = dgl.graph((train_neg_u, train_neg_v), num_nodes=g.num_nodes())\n",
        "\n",
        "test_pos_g = dgl.graph((test_pos_u, test_pos_v), num_nodes=g.num_nodes())\n",
        "test_neg_g = dgl.graph((test_neg_u, test_neg_v), num_nodes=g.num_nodes())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The benefit of treating the pairs of nodes as a graph is that you can\n",
        "use the ``DGLGraph.apply_edges`` method, which conveniently computes new\n",
        "edge features based on the incident nodes’ features and the original\n",
        "edge features (if applicable).\n",
        "\n",
        "DGL provides a set of optimized builtin functions to compute new\n",
        "edge features based on the original node/edge features. For example,\n",
        "``dgl.function.u_dot_v`` computes a dot product of the incident nodes’\n",
        "representations for each edge.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import dgl.function as fn\n",
        "\n",
        "\n",
        "class DotPredictor(nn.Module):\n",
        "    def forward(self, g, h):\n",
        "        with g.local_scope():\n",
        "            g.ndata[\"h\"] = h\n",
        "            # Compute a new edge feature named 'score' by a dot-product between the\n",
        "            # source node feature 'h' and destination node feature 'h'.\n",
        "            g.apply_edges(fn.u_dot_v(\"h\", \"h\", \"score\"))\n",
        "            # u_dot_v returns a 1-element vector for each edge so you need to squeeze it.\n",
        "            return g.edata[\"score\"][:, 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can also write your own function if it is complex.\n",
        "For instance, the following module produces a scalar score on each edge\n",
        "by concatenating the incident nodes’ features and passing it to an MLP.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class MLPPredictor(nn.Module):\n",
        "    def __init__(self, h_feats):\n",
        "        super().__init__()\n",
        "        self.W1 = nn.Linear(h_feats * 2, h_feats)\n",
        "        self.W2 = nn.Linear(h_feats, 1)\n",
        "\n",
        "    def apply_edges(self, edges):\n",
        "        \"\"\"\n",
        "        Computes a scalar score for each edge of the given graph.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        edges :\n",
        "            Has three members ``src``, ``dst`` and ``data``, each of\n",
        "            which is a dictionary representing the features of the\n",
        "            source nodes, the destination nodes, and the edges\n",
        "            themselves.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        dict\n",
        "            A dictionary of new edge features.\n",
        "        \"\"\"\n",
        "        h = torch.cat([edges.src[\"h\"], edges.dst[\"h\"]], 1)\n",
        "        return {\"score\": self.W2(F.relu(self.W1(h))).squeeze(1)}\n",
        "\n",
        "    def forward(self, g, h):\n",
        "        with g.local_scope():\n",
        "            g.ndata[\"h\"] = h\n",
        "            g.apply_edges(self.apply_edges)\n",
        "            return g.edata[\"score\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>The builtin functions are optimized for both speed and memory.\n",
        "   We recommend using builtin functions whenever possible.</p></div>\n",
        "\n",
        "<div class=\"alert alert-info\"><h4>Note</h4><p>If you have read the :doc:`message passing\n",
        "   tutorial <3_message_passing>`, you will notice that the\n",
        "   argument ``apply_edges`` takes has exactly the same form as a message\n",
        "   function in ``update_all``.</p></div>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training loop\n",
        "\n",
        "After you defined the node representation computation and the edge score\n",
        "computation, you can go ahead and define the overall model, loss\n",
        "function, and evaluation metric.\n",
        "\n",
        "The loss function is simply binary cross entropy loss.\n",
        "\n",
        "\\begin{align}\\mathcal{L} = -\\sum_{u\\sim v\\in \\mathcal{D}}\\left( y_{u\\sim v}\\log(\\hat{y}_{u\\sim v}) + (1-y_{u\\sim v})\\log(1-\\hat{y}_{u\\sim v})) \\right)\\end{align}\n",
        "\n",
        "The evaluation metric in this tutorial is AUC.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = GraphSAGE(train_g.ndata[\"feat\"].shape[1], 16)\n",
        "# You can replace DotPredictor with MLPPredictor.\n",
        "# pred = MLPPredictor(16)\n",
        "pred = DotPredictor()\n",
        "\n",
        "\n",
        "def compute_loss(pos_score, neg_score):\n",
        "    scores = torch.cat([pos_score, neg_score])\n",
        "    labels = torch.cat(\n",
        "        [torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]\n",
        "    )\n",
        "    return F.binary_cross_entropy_with_logits(scores, labels)\n",
        "\n",
        "\n",
        "def compute_auc(pos_score, neg_score):\n",
        "    scores = torch.cat([pos_score, neg_score]).numpy()\n",
        "    labels = torch.cat(\n",
        "        [torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]\n",
        "    ).numpy()\n",
        "    return roc_auc_score(labels, scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The training loop goes as follows:\n",
        "\n",
        "<div class=\"alert alert-info\"><h4>Note</h4><p>This tutorial does not include evaluation on a validation\n",
        "   set. In practice you should save and evaluate the best model based on\n",
        "   performance on the validation set.</p></div>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# ----------- 3. set up loss and optimizer -------------- #\n",
        "# in this case, loss will in training loop\n",
        "optimizer = torch.optim.Adam(\n",
        "    itertools.chain(model.parameters(), pred.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "# ----------- 4. training -------------------------------- #\n",
        "all_logits = []\n",
        "for e in range(100):\n",
        "    # forward\n",
        "    h = model(train_g, train_g.ndata[\"feat\"])\n",
        "    pos_score = pred(train_pos_g, h)\n",
        "    neg_score = pred(train_neg_g, h)\n",
        "    loss = compute_loss(pos_score, neg_score)\n",
        "\n",
        "    # backward\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if e % 5 == 0:\n",
        "        print(\"In epoch {}, loss: {}\".format(e, loss))\n",
        "\n",
        "# ----------- 5. check results ------------------------ #\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "with torch.no_grad():\n",
        "    pos_score = pred(test_pos_g, h)\n",
        "    neg_score = pred(test_neg_g, h)\n",
        "    print(\"AUC\", compute_auc(pos_score, neg_score))\n",
        "\n",
        "\n",
        "# Thumbnail credits: Link Prediction with Neo4j, Mark Needham\n",
        "# sphinx_gallery_thumbnail_path = '_static/blitz_4_link_predict.png'"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
