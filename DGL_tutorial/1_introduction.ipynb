{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# DGLを用いたノード分類\n",
        "\n",
        "GNNはグラフ上の多くの機械学習タスクに対して強力なツールである。今回のチュートリアルでは、ノード分類のためにGNNを使用する基本的なワークフローを学ぶ。\n",
        "つまり、グラフ内のノードのカテゴリを予測する。\n",
        "\n",
        "### 目標\n",
        "\n",
        "-  DGL提供のデータセットを読み込む。\n",
        "-  DGL提供のnnモジュールを使用してGNNモデルを構築する。\n",
        "-  ノード分類のためのGNNモデルを学習し、評価する。\n",
        "\n",
        "このチュートリアルは、PyTorchを使用してニューラルネットワークを構築する経験があることを前提としている。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Python=3.11.8\n",
        "2. Pytorch==2.1.2+cu121\n",
        "3. dgl==2.1.0.cu121\n",
        "4. dglgo==0.0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# ライブラリのインポート\n",
        "import os\n",
        "\n",
        "os.environ[\"DGLBACKEND\"] = \"pytorch\" # バックエンドとしてPytorch, Tensorflow, MXNetを指定可能。今回はPytorch\n",
        "import dgl\n",
        "#import dgl.data \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GNNを用いたノード分類の概要\n",
        "グラフデータに対する最も一般的かつ広く用いられているタスクの1つに、ノード分類がある。ノード分類においては、モデルは各ノードのカテゴリを予測する。グラフニューラルネットワーク以前は、多くの手法が接続性 (connectivity) のみ（DeepWalkやnode2vecなど）、あるいは接続性とノード自身の特徴の単純な組み合わせを使用していた。それに対して、GNNは局所的な近傍の接続性と特徴を組み合わせてノード表現を得ることが可能である。\n",
        "\n",
        "[Kipfら](https://arxiv.org/abs/1609.02907)は、ノード分類問題を半教師あり学習によるノード分類タスクとして定式化した例である。ラベル付けされたノードのごく一部だけを使って、グラフニューラルネットワーク（GNN）は他のノードのカテゴリを正確に予測できる。このチュートリアルでは、引用ネットワークであるCoraデータセットを用いて、論文をノードとし、引用をエッジとして、ごく少数のラベルで半教師ありノード分類のためのGNNを構築する方法を示す。タスクは、与えられた論文のカテゴリを予測することだ。各論文ノードは、[論文](https://arxiv.org/abs/1609.02907)のセクション5.2で説明されているように、単語カウントベクトルを特徴として含んでおり、合計が1になるように正規化されている。\n",
        "\n",
        "## Coraデータセットの読み込み\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Coraデータセットは、7つのクラスのいずれかに分類された2708の科学的出版物で構成されている。引用ネットワークは5429のリンクで構成されています。データセット内の各出版物は、辞書の対応する単語の有無を示す0/1値の単語ベクトルによって記述されます。辞書は1433のユニークな単語で構成されています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "Number of categories: 7\n"
          ]
        }
      ],
      "source": [
        "dataset = dgl.data.CoraGraphDataset()\n",
        "print(f\"Number of categories: {dataset.num_classes}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "このチュートリアルで使用されているCoraデータセットは、1つのグラフのみで構成されている。DGLのDatasetオブジェクトは、1つまたは複数のグラフを含むことができる。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "g = dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 10556])"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "edges = torch.cat([g.edges()[0].reshape(1, -1), g.edges()[1].reshape(1, -1)], dim=0)\n",
        "edges.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[   0,    0,    0,  ..., 2707, 2707, 2707],\n",
              "        [ 633, 1862, 2582,  ...,  598, 1473, 2706]])"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "edges"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "DGLグラフは、```ndata```と```edata```と呼ばれる2つの辞書のような属性にノードの特徴とエッジの特徴を格納できる。DGLのCoraデータセットでは、グラフには以下のようなノードの特徴が含まれている：\n",
        "\n",
        "- ``train_mask``: trainingセットにノードが含まれているかどうかを示すブール値のテンソル。\n",
        "\n",
        "- ``val_mask``: validationセットにノードが含まれているかどうかを示すブール値のテンソル。\n",
        "\n",
        "- ``test_mask``: testセットにノードが含まれているかどうかを示すブール値のテンソル。\n",
        "\n",
        "- ``label``: 真のノードのラベル (論文のカテゴリ)。\n",
        "\n",
        "-  ``feat``: ノードの特徴量 (単語カウントベクトル)。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Node features\n",
            "{'feat': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]]), 'label': tensor([3, 4, 4,  ..., 3, 3, 3]), 'test_mask': tensor([False, False, False,  ...,  True,  True,  True]), 'val_mask': tensor([False, False, False,  ..., False, False, False]), 'train_mask': tensor([ True,  True,  True,  ..., False, False, False])}\n",
            "Edge features\n",
            "{}\n"
          ]
        }
      ],
      "source": [
        "print(\"Node features\")\n",
        "print(g.ndata)\n",
        "print(\"Edge features\")\n",
        "print(g.edata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "feat: torch.Size([2708, 1433])\n",
            "label: torch.Size([2708])\n",
            "train_mask: tensor(140)\n",
            "val_mask: tensor(500)\n",
            "test_mask: tensor(1000)\n"
          ]
        }
      ],
      "source": [
        "# 2708はノード数 (論文数)、1433は特徴量の次元数 (単語数)\n",
        "print(\"feat:\", g.ndata[\"feat\"].shape)\n",
        "print(\"label:\", g.ndata[\"label\"].shape)\n",
        "print(\"train_mask:\", g.ndata[\"train_mask\"].sum())\n",
        "print(\"val_mask:\", g.ndata[\"val_mask\"].sum())\n",
        "print(\"test_mask:\", g.ndata[\"test_mask\"].sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "エッジの特徴は、このチュートリアルでは使用されていない。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Graph Convolutional Network (GCN) の構築\n",
        "\n",
        "このチュートリアルでは、2層の[Graph Convolutional Network (GCN)](http://tkipf.github.io/graph-convolutional-networks/)\n",
        "を構築する。各層は、隣接情報を集約して新しいノード表現を計算する。\n",
        "\n",
        "多層のGCNを構築するには、``dgl.nn.GraphConv``モジュールを積み重ねるだけでよい。これは、``torch.nn.Module``を継承している。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from dgl.nn import GraphConv\n",
        "\n",
        "# GCNモデルの定義\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, in_feats, h_feats, num_classes):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GraphConv(in_feats, h_feats)\n",
        "        self.conv2 = GraphConv(h_feats, num_classes)\n",
        "\n",
        "    def forward(self, g, in_feat):\n",
        "        h = self.conv1(g, in_feat)\n",
        "        h = F.relu(h)\n",
        "        h = self.conv2(g, h)\n",
        "        return h\n",
        "\n",
        "# 入力特徴量の次元数、中間層の次元数、出力次元数を指定してモデルのインスタンスを生成。今回は (1433, 16, 7)\n",
        "model = GCN(g.ndata[\"feat\"].shape[1], 16, dataset.num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "DGLは、多くの一般的な隣接情報集約モジュールの実装を提供している。1行のコードで簡単に呼び出すことができる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GCNの学習\n",
        "\n",
        "GCNの学習は、他のPyTorchニューラルネットワークの学習と同様である。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In epoch 0, loss: 1.946, val acc: 0.142 (best 0.142), test acc: 0.150 (best 0.150)\n",
            "In epoch 5, loss: 1.894, val acc: 0.340 (best 0.358), test acc: 0.359 (best 0.360)\n",
            "In epoch 10, loss: 1.816, val acc: 0.428 (best 0.428), test acc: 0.467 (best 0.467)\n",
            "In epoch 15, loss: 1.713, val acc: 0.514 (best 0.514), test acc: 0.541 (best 0.541)\n",
            "In epoch 20, loss: 1.587, val acc: 0.560 (best 0.560), test acc: 0.587 (best 0.587)\n",
            "In epoch 25, loss: 1.440, val acc: 0.642 (best 0.642), test acc: 0.636 (best 0.636)\n",
            "In epoch 30, loss: 1.278, val acc: 0.660 (best 0.660), test acc: 0.670 (best 0.670)\n",
            "In epoch 35, loss: 1.106, val acc: 0.674 (best 0.676), test acc: 0.704 (best 0.698)\n",
            "In epoch 40, loss: 0.934, val acc: 0.716 (best 0.716), test acc: 0.728 (best 0.728)\n",
            "In epoch 45, loss: 0.770, val acc: 0.732 (best 0.732), test acc: 0.732 (best 0.732)\n",
            "In epoch 50, loss: 0.624, val acc: 0.744 (best 0.744), test acc: 0.746 (best 0.741)\n",
            "In epoch 55, loss: 0.499, val acc: 0.752 (best 0.752), test acc: 0.751 (best 0.751)\n",
            "In epoch 60, loss: 0.397, val acc: 0.752 (best 0.756), test acc: 0.751 (best 0.752)\n",
            "In epoch 65, loss: 0.317, val acc: 0.754 (best 0.756), test acc: 0.753 (best 0.752)\n",
            "In epoch 70, loss: 0.254, val acc: 0.758 (best 0.760), test acc: 0.759 (best 0.758)\n",
            "In epoch 75, loss: 0.205, val acc: 0.758 (best 0.760), test acc: 0.760 (best 0.758)\n",
            "In epoch 80, loss: 0.167, val acc: 0.754 (best 0.760), test acc: 0.763 (best 0.758)\n",
            "In epoch 85, loss: 0.138, val acc: 0.758 (best 0.760), test acc: 0.766 (best 0.758)\n",
            "In epoch 90, loss: 0.116, val acc: 0.762 (best 0.762), test acc: 0.769 (best 0.766)\n",
            "In epoch 95, loss: 0.098, val acc: 0.764 (best 0.764), test acc: 0.766 (best 0.767)\n"
          ]
        }
      ],
      "source": [
        "def train(g, model):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "    best_val_acc = 0\n",
        "    best_test_acc = 0\n",
        "\n",
        "    features = g.ndata[\"feat\"]\n",
        "    labels = g.ndata[\"label\"]\n",
        "    train_mask = g.ndata[\"train_mask\"]\n",
        "    val_mask = g.ndata[\"val_mask\"]\n",
        "    test_mask = g.ndata[\"test_mask\"]\n",
        "\n",
        "    \n",
        "    for e in range(100):\n",
        "        # Forward\n",
        "        logits = model(g, features)\n",
        "\n",
        "        # Compute prediction\n",
        "        pred = logits.argmax(1)\n",
        "\n",
        "        # Compute accuracy on training/validation/test\n",
        "        train_acc = (pred[train_mask] == labels[train_mask]).float().mean()\n",
        "        val_acc = (pred[val_mask] == labels[val_mask]).float().mean()\n",
        "        test_acc = (pred[test_mask] == labels[test_mask]).float().mean()\n",
        "        \n",
        "        # Compute loss\n",
        "        # Note that you should only compute the losses of the nodes in the training set.\n",
        "        loss = F.cross_entropy(logits[train_mask], labels[train_mask])\n",
        "\n",
        "        # Save the best validation accuracy and the corresponding test accuracy.\n",
        "        if best_val_acc < val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_test_acc = test_acc\n",
        "\n",
        "        # Backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if e % 5 == 0:\n",
        "            print(\n",
        "                f\"In epoch {e}, loss: {loss:.3f}, val acc: {val_acc:.3f} (best {best_val_acc:.3f}), test acc: {test_acc:.3f} (best {best_test_acc:.3f})\"\n",
        "            )\n",
        "\n",
        "model = GCN(g.ndata[\"feat\"].shape[1], 16, dataset.num_classes)\n",
        "train(g, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GPUでの学習\n",
        "GPUでの学習には、Pytorchと同様モデルとグラフの両方を``to``メソッドを使ってGPUに配置する必要がある。\n",
        "\n",
        "```python\n",
        "\n",
        "   g = g.to('cuda')\n",
        "   model = GCN(g.ndata['feat'].shape[1], 16, dataset.num_classes).to('cuda')\n",
        "   train(g, model)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In epoch 0, loss: 1.947, val acc: 0.102 (best 0.102), test acc: 0.114 (best 0.114)\n",
            "In epoch 5, loss: 1.903, val acc: 0.336 (best 0.406), test acc: 0.335 (best 0.433)\n",
            "In epoch 10, loss: 1.826, val acc: 0.402 (best 0.406), test acc: 0.429 (best 0.433)\n",
            "In epoch 15, loss: 1.729, val acc: 0.540 (best 0.540), test acc: 0.575 (best 0.575)\n",
            "In epoch 20, loss: 1.607, val acc: 0.612 (best 0.612), test acc: 0.626 (best 0.626)\n",
            "In epoch 25, loss: 1.466, val acc: 0.642 (best 0.642), test acc: 0.658 (best 0.658)\n",
            "In epoch 30, loss: 1.307, val acc: 0.676 (best 0.676), test acc: 0.690 (best 0.690)\n",
            "In epoch 35, loss: 1.138, val acc: 0.700 (best 0.700), test acc: 0.708 (best 0.708)\n",
            "In epoch 40, loss: 0.968, val acc: 0.722 (best 0.722), test acc: 0.727 (best 0.727)\n",
            "In epoch 45, loss: 0.806, val acc: 0.726 (best 0.728), test acc: 0.743 (best 0.734)\n",
            "In epoch 50, loss: 0.661, val acc: 0.746 (best 0.746), test acc: 0.747 (best 0.747)\n",
            "In epoch 55, loss: 0.535, val acc: 0.748 (best 0.750), test acc: 0.748 (best 0.748)\n",
            "In epoch 60, loss: 0.431, val acc: 0.744 (best 0.750), test acc: 0.753 (best 0.748)\n",
            "In epoch 65, loss: 0.347, val acc: 0.756 (best 0.756), test acc: 0.757 (best 0.757)\n",
            "In epoch 70, loss: 0.281, val acc: 0.766 (best 0.766), test acc: 0.759 (best 0.758)\n",
            "In epoch 75, loss: 0.229, val acc: 0.768 (best 0.768), test acc: 0.762 (best 0.759)\n",
            "In epoch 80, loss: 0.188, val acc: 0.774 (best 0.774), test acc: 0.766 (best 0.766)\n",
            "In epoch 85, loss: 0.156, val acc: 0.770 (best 0.774), test acc: 0.765 (best 0.766)\n",
            "In epoch 90, loss: 0.131, val acc: 0.766 (best 0.774), test acc: 0.764 (best 0.766)\n",
            "In epoch 95, loss: 0.111, val acc: 0.766 (best 0.774), test acc: 0.765 (best 0.766)\n"
          ]
        }
      ],
      "source": [
        "# GPUを使って学習\n",
        "g = g.to('cuda')\n",
        "model = GCN(g.ndata['feat'].shape[1], 16, dataset.num_classes).to('cuda')\n",
        "train(g, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "今回はGPUよりもCPUのほう速かった。おそらく学習が短すぎてテンソルとモデルの転送にかかる時間が大きかったためだと思われる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What’s next?\n",
        "\n",
        "-  :doc:`How does DGL represent a graph <2_dglgraph>`?\n",
        "-  :doc:`Write your own GNN module <3_message_passing>`.\n",
        "-  :doc:`Link prediction (predicting existence of edges) on full\n",
        "   graph <4_link_predict>`.\n",
        "-  :doc:`Graph classification <5_graph_classification>`.\n",
        "-  :doc:`Make your own dataset <6_load_data>`.\n",
        "-  `The list of supported graph convolution\n",
        "   modules <apinn-pytorch>`.\n",
        "-  `The list of datasets provided by DGL <apidata>`.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Thumbnail credits: Stanford CS224W Notes\n",
        "# sphinx_gallery_thumbnail_path = '_static/blitz_1_introduction.png'"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
