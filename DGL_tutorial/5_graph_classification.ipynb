{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# グラフ分類のためのGNN学習\n",
        "\n",
        "このチュートリアルでは、以下のことができるようになる\n",
        "\n",
        "- DGLが提供するグラフ分類データセットをロードする\n",
        "- *readout*関数が何をするかを理解する\n",
        "- グラフのミニバッチを作成して使用する方法を理解する\n",
        "- GNNベースのグラフ分類モデルを構築する\n",
        "- DGLが提供するデータセットでモデルをトレーニングおよび評価する\n",
        "\n",
        "(Time estimate: 18 minutes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# ライブラリのインポート\n",
        "import os\n",
        "\n",
        "os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
        "import dgl\n",
        "import dgl.data\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GNNを使ったグラフ分類の概要\n",
        "\n",
        "グラフ分類（または回帰）は、ノードとエッジの特徴量が与えられた単一のグラフのグラフレベルの特性を予測するモデルが必要とされる。分子特性予測はその一例である。\n",
        "\n",
        "このチュートリアルでは、グラフ分類モデルを学習させる方法を以下の論文の小規模データセット[How Powerful Are Graph Neural Networks](https://arxiv.org/abs/1810.00826)を用いて紹介する。\n",
        "\n",
        "## データの読み込み\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# ノード数が10から500までの10000個のグラフを持つ合成データセットを生成します。\n",
        "dataset = dgl.data.GINDataset(\"PROTEINS\", self_loop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset(\"PROTEINS\", num_graphs=1113, save_path=/home/takutoyo/.dgl/PROTEINS_0c2c49a1)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "データセットは、ノード特徴量と単一のラベルを持つグラフのセットである。``GINDataset``オブジェクトの``dim_nfeats``および``gclasses``属性には、ノード特徴量の次元とグラフカテゴリの数が含まれる。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Node feature dimensionality: 3\n",
            "Number of graph categories: 2\n"
          ]
        }
      ],
      "source": [
        "print(\"Node feature dimensionality:\", dataset.dim_nfeats)\n",
        "print(\"Number of graph categories:\", dataset.gclasses)\n",
        "\n",
        "\n",
        "from dgl.dataloading import GraphDataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loaderの定義\n",
        "\n",
        "グラフ分類データセットは通常、グラフのセットとグラフレベルのラベルを含む2つの要素を持つ。画像分類タスクと同様に、データセットが十分に大きい場合、ミニバッチでトレーニングする必要がある。画像分類や自然言語のモデルを学習させる場合、データセットを反復処理するために``DataLoader``を使用する。DGLでは、``GraphDataLoader``を使用できる。\n",
        "\n",
        "``GraphDataLoader``は、グラフのミニバッチを生成するためのデータローダーである。``GraphDataLoader``は、``DataLoader``と同様に使用されるが、グラフのミニバッチを生成するために``collate``関数を使用する。\n",
        "\n",
        "``collate``関数は、グラフのリストを入力として受け取り、バッチ化されたグラフを返す。バッチ化されたグラフは、**単一の大きなグラフになる**。各元のグラフは、バッチ化されたグラフのノードおよびエッジのインデックスを保持するため、バッチ化されたグラフのノードおよびエッジの特徴量テンソルは、元のグラフのノードおよびエッジのインデックスに対応する。\n",
        "\n",
        "[torch.utils.data.sampler](https://pytorch.org/docs/stable/data.html#data-loading-order-and-sampler)で提供されるさまざまなデータセットサンプラーを使用することもできる。例えば、このチュートリアルでは、トレーニング``GraphDataLoader``とテスト``GraphDataLoader``を作成し、``SubsetRandomSampler``を使用して、データセットのサブセットからのみサンプリングするようにPyTorchに指示する。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "num_examples = len(dataset)\n",
        "num_train = int(num_examples * 0.8)\n",
        "\n",
        "train_sampler = SubsetRandomSampler(torch.arange(num_train))\n",
        "test_sampler = SubsetRandomSampler(torch.arange(num_train, num_examples))\n",
        "\n",
        "train_dataloader = GraphDataLoader(\n",
        "    dataset, sampler=train_sampler, batch_size=5, drop_last=False\n",
        ")\n",
        "test_dataloader = GraphDataLoader(\n",
        "    dataset, sampler=test_sampler, batch_size=5, drop_last=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``GraphDataLoader``を作成して、その中をイテレートしてみると、どのようなものが得られるか確認できる。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Graph(num_nodes=215, num_edges=1041,\n",
            "      ndata_schemes={'attr': Scheme(shape=(3,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64)}\n",
            "      edata_schemes={}), tensor([0, 1, 0, 0, 0])]\n"
          ]
        }
      ],
      "source": [
        "it = iter(train_dataloader)\n",
        "batch = next(it)\n",
        "print(batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``dataset``の各要素にはグラフとラベルが含まれているため、``GraphDataLoader``は各イテレーションで2つのオブジェクトを返す。最初の要素はバッチ化されたグラフであり、2番目の要素は単にミニバッチ内の各グラフのカテゴリを表すラベルベクトルである。次に、バッチ化されたグラフについて説明する。\n",
        "\n",
        "## DGLのバッチ化されたグラフ\n",
        "\n",
        "各ミニバッチでは、サンプリングされたグラフは``dgl.batch``を介して単一の大きなバッチ化されたグラフに結合される。単一の大きなバッチ化されたグラフは、元のグラフを個別に接続されたコンポーネントとしてマージし、ノードとエッジの特徴量を連結する。この大きなグラフも``DGLGraph``インスタンスである（したがって、[こちら](2_dglgraph.ipynb)のように通常の``DGLGraph``オブジェクトとして扱うことができる）。ただし、各グラフ要素のノード数とエッジ数など、元のグラフを回復するために必要な情報が含まれている。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of nodes for each graph element in the batch: tensor([34, 15, 46, 84, 36])\n",
            "Number of edges for each graph element in the batch: tensor([156,  71, 228, 408, 178])\n",
            "The original graphs in the minibatch:\n",
            "[Graph(num_nodes=34, num_edges=156,\n",
            "      ndata_schemes={'attr': Scheme(shape=(3,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64)}\n",
            "      edata_schemes={}), Graph(num_nodes=15, num_edges=71,\n",
            "      ndata_schemes={'attr': Scheme(shape=(3,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64)}\n",
            "      edata_schemes={}), Graph(num_nodes=46, num_edges=228,\n",
            "      ndata_schemes={'attr': Scheme(shape=(3,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64)}\n",
            "      edata_schemes={}), Graph(num_nodes=84, num_edges=408,\n",
            "      ndata_schemes={'attr': Scheme(shape=(3,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64)}\n",
            "      edata_schemes={}), Graph(num_nodes=36, num_edges=178,\n",
            "      ndata_schemes={'attr': Scheme(shape=(3,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64)}\n",
            "      edata_schemes={})]\n"
          ]
        }
      ],
      "source": [
        "batched_graph, labels = batch\n",
        "print(\n",
        "    \"Number of nodes for each graph element in the batch:\",\n",
        "    batched_graph.batch_num_nodes(),\n",
        ")\n",
        "print(\n",
        "    \"Number of edges for each graph element in the batch:\",\n",
        "    batched_graph.batch_num_edges(),\n",
        ")\n",
        "\n",
        "# 元のグラフ要素をミニバッチから復元する\n",
        "graphs = dgl.unbatch(batched_graph)\n",
        "print(\"The original graphs in the minibatch:\")\n",
        "print(graphs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## モデルの定義\n",
        "\n",
        "\n",
        "\n",
        "このチュートリアルでは、2層の[Graph Convolutional Network](http://tkipf.github.io/graph-convolutional-networks/)（GCN）を構築する。各レイヤーは、隣接情報を集約して新しいノード表現を計算する。もし、:doc:`introduction <1_introduction>`を読んだことがあれば、2つの違いがわかるだろう。\n",
        "\n",
        "- ここで実施するタスクは、*グラフ全体*のための単一のカテゴリを予測することであり、各ノードについてではない。そのため、すべてのノードとエッジの表現を集約して新しいグラフレベルの表現を計算する必要がある。このようなプロセスは、一般的に*readout*と呼ばれる。最もシンプルなreadout手法は、``dgl.mean_nodes()``を使用してグラフのノード特徴量を平均することである。\n",
        "\n",
        "- モデルに入力されるグラフは、``GraphDataLoader``によって生成されるバッチ化されたグラフである。DGLが提供するreadout関数は、バッチ化されたグラフを処理できるため、各ミニバッチ要素に対して1つの表現を返す。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from dgl.nn import GraphConv\n",
        "from dgl.nn import GINConv\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, in_feats, h_feats, num_classes):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GraphConv(in_feats, h_feats)\n",
        "        self.conv2 = GraphConv(h_feats, num_classes)\n",
        "\n",
        "    def forward(self, g, in_feat):\n",
        "        h = self.conv1(g, in_feat)\n",
        "        h = F.relu(h)\n",
        "        h = self.conv2(g, h)\n",
        "        g.ndata[\"h\"] = h\n",
        "        return dgl.mean_nodes(g, \"h\")\n",
        "    \n",
        "class GIN(nn.Module):\n",
        "    def __init__(self, in_feats, h_feats, num_classes):\n",
        "        super(GIN, self).__init__()\n",
        "        self.conv1 = GINConv(nn.Linear(in_feats, h_feats), \"sum\")\n",
        "        self.conv2 = GINConv(nn.Linear(h_feats, num_classes), \"sum\")\n",
        "\n",
        "    def forward(self, g, in_feat):\n",
        "        h = self.conv1(g, in_feat)\n",
        "        h = F.relu(h)\n",
        "        h = self.conv2(g, h)\n",
        "        g.ndata[\"h\"] = h\n",
        "        return dgl.mean_nodes(g, \"h\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 学習ループ\n",
        "\n",
        "学習ループでは、トレーニングセットを``GraphDataLoader``オブジェクトで反復処理し、勾配を計算する。これは画像分類や言語モデリングと同様である。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/9 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9/9 [00:08<00:00,  1.10it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm \n",
        "# Create the model with given dimensions\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# model = GCN(dataset.dim_nfeats, 16, dataset.gclasses).to(device)\n",
        "model = GIN(dataset.dim_nfeats, 16, dataset.gclasses).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in tqdm(range(20)):\n",
        "    for batched_graph, labels in train_dataloader:\n",
        "        batched_graph, labels = batched_graph.to(device), labels.to(device)\n",
        "        pred = model(batched_graph, batched_graph.ndata[\"attr\"].float())\n",
        "        loss = F.cross_entropy(pred, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "num_correct = 0\n",
        "num_tests = 0\n",
        "for batched_graph, labels in test_dataloader:\n",
        "    batched_graph, labels = batched_graph.to(device), labels.to(device)\n",
        "    pred = model(batched_graph, batched_graph.ndata[\"attr\"].float())\n",
        "    num_correct += (pred.argmax(1) == labels).sum().item()\n",
        "    num_tests += len(labels)\n",
        "\n",
        "print(\"Test accuracy:\", num_correct / num_tests)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What’s next\n",
        "\n",
        "-  See [GIN\n",
        "   example](https://github.com/dmlc/dgl/tree/master/examples/pytorch/gin)_\n",
        "   for an end-to-end graph classification model.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Thumbnail credits: DGL\n",
        "# sphinx_gallery_thumbnail_path = '_static/blitz_5_graph_classification.png'"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
