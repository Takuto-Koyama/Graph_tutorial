{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jv-tHPvR-JKa"
      },
      "source": [
        "# 簡単に作れる　Graph Transformer \n",
        "\n",
        "**Transformer** [(Vaswani et al. 2017)](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html) は、自然言語処理とコンピュータビジョンにおいて効果的な学習アーキテクチャであることが証明されている。近年、Transformerをグラフ学習応用するような試みが盛んに行われていおり、多くの実践的なタスク、例えばグラフ特性予測において成功を収めつつある。 [Dwivedi et al. (2020)](https://arxiv.org/abs/2012.09699) は、最初にTransformerのニューラルアーキテクチャをグラフ構造データに一般化した。ここでは、DGLのsparse matrix APIを用いてそのようなGraph Transformerを構築する方法を紹介する。\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dmlc/dgl/blob/master/notebooks/sparse/graph_transformer.ipynb) [![GitHub](https://img.shields.io/badge/-View%20on%20GitHub-181717?logo=github&logoColor=ffffff)](https://github.com/dmlc/dgl/blob/master/notebooks/sparse/graph_transformer.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8wIJZQqODy-7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DGL installed!\n"
          ]
        }
      ],
      "source": [
        "# パッケージのインストール\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "os.environ['DGLBACKEND'] = \"pytorch\"\n",
        "\n",
        "# Uncomment below to install required packages. If the CUDA version is not 11.8,\n",
        "# check the https://www.dgl.ai/pages/start.html to find the supported CUDA\n",
        "# version and corresponding command to install DGL.\n",
        "#!pip install dgl -f https://data.dgl.ai/wheels/cu118/repo.html > /dev/null\n",
        "#!pip install ogb >/dev/null\n",
        "\n",
        "try:\n",
        "    import dgl\n",
        "    installed = True\n",
        "except ImportError:\n",
        "    installed = False\n",
        "print(\"DGL installed!\" if installed else \"Failed to install DGL!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOpFdtLI-JKb"
      },
      "source": [
        "## Sparse Multi-head Attention\n",
        "\n",
        "Transformerにおける全ペアのスケールドット積アテンションメカニズムを思い出してほしい:\n",
        "\n",
        "$$\\text{Attn}=\\text{softmax}(\\dfrac{QK^T} {\\sqrt{d}})V,$$\n",
        "\n",
        "一方でグラフトランスフォーマー（GT）モデルは、Sparse Multi-head Attention ブロックを採用している:\n",
        "\n",
        "$$\\text{SparseAttn}(Q, K, V, A) = \\text{softmax}(\\frac{(QK^T) \\circ A}{\\sqrt{d}})V,$$\n",
        "\n",
        "ここで、 $Q, K, V ∈\\mathbb{R}^{N\\times d}$ はそれぞれクエリ特徴、キー特徴、およびバリュー特徴である。 $A\\in[0,1]^{N\\times N}$ は入力グラフの隣接行列である。 $(QK^T)\\circ A$ は、クエリ行列とキー行列の積の後に疎隣接行列とのアダマール積（要素ごとの積）が行われることを意味する。下図に示されているように:\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"./images/sparseattn.png\" width=\"500\">\n",
        "</p>\n",
        "\n",
        "本質的には、$A$ の疎性に従って接続されたノード間のアテンションスコアのみが計算される。この操作は *Sampled Dense Dense Matrix Multiplication (SDDMM)* とも呼ばれる。  \n",
        "\n",
        "DGLの [batched SDDMM API](https://docs.dgl.ai/en/latest/generated/dgl.sparse.bsddmm.html) を利用することで、複数のアテンションヘッド（異なる表現部分空間）の計算を並列化することができる。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dh7zc5v0-JKb"
      },
      "outputs": [],
      "source": [
        "import dgl\n",
        "import dgl.nn as dglnn\n",
        "import dgl.sparse as dglsp\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from dgl.data import AsGraphPredDataset\n",
        "from dgl.dataloading import GraphDataLoader\n",
        "from ogb.graphproppred import collate_dgl, DglGraphPropPredDataset, Evaluator\n",
        "from ogb.graphproppred.mol_encoder import AtomEncoder\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Sparse Multi-head Attention Moduleの実装\n",
        "class SparseMHA(nn.Module):\n",
        "    \"\"\"Sparse Multi-head Attention Module\"\"\"\n",
        "\n",
        "    def __init__(self, hidden_size=80, num_heads=8):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = hidden_size // num_heads\n",
        "        self.scaling = self.head_dim**-0.5\n",
        "\n",
        "        self.q_proj = nn.Linear(hidden_size, hidden_size)\n",
        "        self.k_proj = nn.Linear(hidden_size, hidden_size)\n",
        "        self.v_proj = nn.Linear(hidden_size, hidden_size)\n",
        "        self.out_proj = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, A, h):\n",
        "        N = len(h)\n",
        "        # [N, dh, nh]\n",
        "        q = self.q_proj(h).reshape(N, self.head_dim, self.num_heads)\n",
        "        q *= self.scaling\n",
        "        # [N, dh, nh]\n",
        "        k = self.k_proj(h).reshape(N, self.head_dim, self.num_heads)\n",
        "        # [N, dh, nh]\n",
        "        v = self.v_proj(h).reshape(N, self.head_dim, self.num_heads)\n",
        "\n",
        "        ######################################################################\n",
        "        # (HIGHLIGHT) Compute the multi-head attention with Sparse Matrix API\n",
        "        ######################################################################\n",
        "        # dglspのbsddmm関数を使って、SDDMMをバッチ単位で計算する\n",
        "        attn = dglsp.bsddmm(A, q, k.transpose(1, 0))  # (sparse) [N, N, nh]\n",
        "        # Sparse softmax by default applies on the last sparse dimension.\n",
        "        attn = attn.softmax()  # (sparse) [N, N, nh]\n",
        "        out = dglsp.bspmm(attn, v)  # [N, dh, nh]\n",
        "\n",
        "        return self.out_proj(out.reshape(N, -1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_Fm6Lrx-JKc"
      },
      "source": [
        "## Graph Transformer Layer\n",
        "\n",
        "GT層は、マルチヘッドアテンション、バッチノーマライゼーション、フィードフォワードネットワークで構成されており、これらは通常のトランスフォーマーのように残差リンクで接続されている。\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"./images/gt_layers.png\" width=\"300\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "M6h7JVWT-JKd"
      },
      "outputs": [],
      "source": [
        "class GTLayer(nn.Module):\n",
        "    \"\"\"Graph Transformer Layer\"\"\"\n",
        "\n",
        "    def __init__(self, hidden_size=80, num_heads=8):\n",
        "        super().__init__()\n",
        "        self.MHA = SparseMHA(hidden_size=hidden_size, num_heads=num_heads)\n",
        "        self.batchnorm1 = nn.BatchNorm1d(hidden_size)\n",
        "        self.batchnorm2 = nn.BatchNorm1d(hidden_size)\n",
        "        self.FFN1 = nn.Linear(hidden_size, hidden_size * 2)\n",
        "        self.FFN2 = nn.Linear(hidden_size * 2, hidden_size)\n",
        "\n",
        "    def forward(self, A, h):\n",
        "        h1 = h\n",
        "        h = self.MHA(A, h)\n",
        "        h = self.batchnorm1(h + h1)\n",
        "\n",
        "        h2 = h\n",
        "        h = self.FFN2(F.relu(self.FFN1(h)))\n",
        "        h = h2 + h\n",
        "\n",
        "        return self.batchnorm2(h)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t40DhVjI-JKd"
      },
      "source": [
        "## Graph Transformer Model\n",
        "\n",
        "GTモデルは、GT層を積み重ねて構築されます。通常のTransformerの入力位置エンコーディングは、ラプラシアン位置エンコーディング[(Dwivedi et al. 2020)](https://arxiv.org/abs/2003.00982)に置き換えられている。グラフレベルの予測タスクのために、同じグラフのノード特徴を集約するためにGT層の上に追加のpooling層が積み重ねられている。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UrjvEBrF-JKe"
      },
      "outputs": [],
      "source": [
        "class GTModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        out_size,\n",
        "        hidden_size=80,\n",
        "        pos_enc_size=2,\n",
        "        num_layers=8,\n",
        "        num_heads=8,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.atom_encoder = AtomEncoder(hidden_size)\n",
        "        self.pos_linear = nn.Linear(pos_enc_size, hidden_size)\n",
        "        self.layers = nn.ModuleList(\n",
        "            [GTLayer(hidden_size, num_heads) for _ in range(num_layers)]\n",
        "        )\n",
        "        self.pooler = dglnn.SumPooling()\n",
        "        self.predictor = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size // 2, hidden_size // 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size // 4, out_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, g, X, pos_enc):\n",
        "        indices = torch.stack(g.edges())\n",
        "        N = g.num_nodes()\n",
        "        A = dglsp.spmatrix(indices, shape=(N, N)) # 疎行列（隣接行列）の作成\n",
        "        h = self.atom_encoder(X) + self.pos_linear(pos_enc) # 位置エンコーディングを加算\n",
        "        for layer in self.layers:\n",
        "            h = layer(A, h) # GTLayerをnum_layers回繰り返す\n",
        "        h = self.pooler(g, h) # プーリング\n",
        "\n",
        "        return self.predictor(h)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdrPU18I-JKe"
      },
      "source": [
        "## 学習\n",
        "\n",
        "今回は、GTモデルを[ogbg-molhiv](https://ogb.stanford.edu/docs/graphprop/#ogbg-mol) ベンチマークで学習する。各グラフのラプラシアン位置エンコーディングは、モデルへの入力の一部として事前に計算されている（APIは[こちら](https://docs.dgl.ai/en/latest/generated/dgl.laplacian_pe.html)）。\n",
        "\n",
        "*デモをより高速に実行するために、データセットをダウンサンプリングしていることに注意してください。フルデータセットでのパフォーマンスについては、*[*サンプルスクリプト*](https://github.com/dmlc/dgl/blob/master/examples/sparse/graph_transformer.py)*を参照してください。*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V41i0w-9-JKe",
        "outputId": "15343d1a-a32d-4677-d053-d9da96910f43"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing Laplacian PE:   0%|          | 0/4000 [00:00<?, ?it/s]/home/takutoyo/.pyenv/versions/miniconda3-latest/envs/dgl_env/lib/python3.11/site-packages/dgl/transforms/functional.py:3725: DGLWarning: dgl.laplacian_pe will be deprecated. Use dgl.lap_pe please.\n",
            "  dgl_warning(\"dgl.laplacian_pe will be deprecated. Use dgl.lap_pe please.\")\n",
            "Computing Laplacian PE: 100%|██████████| 4000/4000 [00:08<00:00, 475.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 000, Loss: 0.4006, Val: 0.3367, Test: 0.3609\n",
            "Epoch: 001, Loss: 0.2118, Val: 0.3531, Test: 0.3778\n",
            "Epoch: 002, Loss: 0.1763, Val: 0.4515, Test: 0.4052\n",
            "Epoch: 003, Loss: 0.1601, Val: 0.5234, Test: 0.4207\n",
            "Epoch: 004, Loss: 0.1520, Val: 0.6077, Test: 0.4546\n",
            "Epoch: 005, Loss: 0.1353, Val: 0.6955, Test: 0.5026\n",
            "Epoch: 006, Loss: 0.1203, Val: 0.6491, Test: 0.4770\n",
            "Epoch: 007, Loss: 0.1114, Val: 0.7394, Test: 0.5912\n",
            "Epoch: 008, Loss: 0.1001, Val: 0.6127, Test: 0.3912\n",
            "Epoch: 009, Loss: 0.0894, Val: 0.7504, Test: 0.6017\n",
            "Epoch: 010, Loss: 0.0835, Val: 0.7043, Test: 0.5280\n",
            "Epoch: 011, Loss: 0.0771, Val: 0.7186, Test: 0.4636\n",
            "Epoch: 012, Loss: 0.0645, Val: 0.6442, Test: 0.3509\n",
            "Epoch: 013, Loss: 0.0506, Val: 0.7130, Test: 0.3913\n",
            "Epoch: 014, Loss: 0.0421, Val: 0.7165, Test: 0.4508\n",
            "Epoch: 015, Loss: 0.0704, Val: 0.5453, Test: 0.4081\n",
            "Epoch: 016, Loss: 0.0483, Val: 0.6208, Test: 0.4531\n",
            "Epoch: 017, Loss: 0.0428, Val: 0.6330, Test: 0.3983\n",
            "Epoch: 018, Loss: 0.0289, Val: 0.7081, Test: 0.4750\n",
            "Epoch: 019, Loss: 0.0249, Val: 0.5354, Test: 0.3514\n"
          ]
        }
      ],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, dataloader, evaluator, device):\n",
        "    model.eval()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    for batched_g, labels in dataloader:\n",
        "        batched_g, labels = batched_g.to(device), labels.to(device)\n",
        "        y_hat = model(batched_g, batched_g.ndata[\"feat\"], batched_g.ndata[\"PE\"])\n",
        "        y_true.append(labels.view(y_hat.shape).detach().cpu())\n",
        "        y_pred.append(y_hat.detach().cpu())\n",
        "    y_true = torch.cat(y_true, dim=0).numpy()\n",
        "    y_pred = torch.cat(y_pred, dim=0).numpy()\n",
        "    input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n",
        "    return evaluator.eval(input_dict)[\"rocauc\"]\n",
        "\n",
        "\n",
        "def train(model, dataset, evaluator, device):\n",
        "    train_dataloader = GraphDataLoader(\n",
        "        dataset[dataset.train_idx],\n",
        "        batch_size=256,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_dgl,\n",
        "    )\n",
        "    valid_dataloader = GraphDataLoader(\n",
        "        dataset[dataset.val_idx], batch_size=256, collate_fn=collate_dgl\n",
        "    )\n",
        "    test_dataloader = GraphDataLoader(\n",
        "        dataset[dataset.test_idx], batch_size=256, collate_fn=collate_dgl\n",
        "    )\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    num_epochs = 20\n",
        "    scheduler = optim.lr_scheduler.StepLR(\n",
        "        optimizer, step_size=num_epochs, gamma=0.5\n",
        "    )\n",
        "    loss_fcn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        for batched_g, labels in train_dataloader:\n",
        "            batched_g, labels = batched_g.to(device), labels.to(device)\n",
        "            logits = model(\n",
        "                batched_g, batched_g.ndata[\"feat\"], batched_g.ndata[\"PE\"]\n",
        "            )\n",
        "            loss = loss_fcn(logits, labels.float())\n",
        "            total_loss += loss.item()\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        scheduler.step()\n",
        "        avg_loss = total_loss / len(train_dataloader)\n",
        "        val_metric = evaluate(model, valid_dataloader, evaluator, device)\n",
        "        test_metric = evaluate(model, test_dataloader, evaluator, device)\n",
        "        print(\n",
        "            f\"Epoch: {epoch:03d}, Loss: {avg_loss:.4f}, \"\n",
        "            f\"Val: {val_metric:.4f}, Test: {test_metric:.4f}\"\n",
        "        )\n",
        "\n",
        "\n",
        "# Training device.\n",
        "#dev = torch.device(\"cpu\")\n",
        "# Uncomment the code below to train on GPU. Be sure to install DGL with CUDA support.\n",
        "dev = torch.device(\"cuda:0\") # GPUで学習\n",
        "\n",
        "# データセットの読み込み\n",
        "pos_enc_size = 8\n",
        "dataset = AsGraphPredDataset(\n",
        "    DglGraphPropPredDataset(\"ogbg-molhiv\", \"./data/OGB\")\n",
        ")\n",
        "evaluator = Evaluator(\"ogbg-molhiv\")\n",
        "\n",
        "# データセットのサンプリング\n",
        "import random\n",
        "random.seed(42)\n",
        "train_size = len(dataset.train_idx)\n",
        "val_size = len(dataset.val_idx)\n",
        "test_size = len(dataset.test_idx)\n",
        "dataset.train_idx = dataset.train_idx[\n",
        "    torch.LongTensor(random.sample(range(train_size), 2000))\n",
        "]\n",
        "dataset.val_idx = dataset.val_idx[\n",
        "    torch.LongTensor(random.sample(range(val_size), 1000))\n",
        "]\n",
        "dataset.test_idx = dataset.test_idx[\n",
        "    torch.LongTensor(random.sample(range(test_size), 1000))\n",
        "]\n",
        "\n",
        "# ラプラシアン位置エンコーディングの計算\n",
        "indices = torch.cat([dataset.train_idx, dataset.val_idx, dataset.test_idx])\n",
        "for idx in tqdm(indices, desc=\"Computing Laplacian PE\"):\n",
        "    g, _ = dataset[idx]\n",
        "    g.ndata[\"PE\"] = dgl.laplacian_pe(g, k=pos_enc_size, padding=True) # ラプラシアン位置エンコーディング\n",
        "\n",
        "# モデルの初期化\n",
        "out_size = dataset.num_tasks\n",
        "model = GTModel(out_size=out_size, pos_enc_size=pos_enc_size).to(dev)\n",
        "\n",
        "# 学習の開始\n",
        "train(model, dataset, evaluator, dev)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
